#! /usr/bin/env python -tt -u #-3#"""Run basic GRB processing on Fermi/GBM data.##   This script runs a simple GRB analysis on a set of Fermi/GBM event#   files (.fit) for a given trigger.  The output is one FITS light#   curve (.lc) file and one FITS PHA2 (.pha) file for each input#   detector event file.  A spectral fit is performed.##"""import mathimport os, shutilimport reimport subprocessimport sysimport GtAppimport numpyimport pyfitsfrom GtBurst import dataHandlingtry:  import xspec  XSPEC_AVAILABLE = Trueexcept:  XSPEC_AVAILABLE = False####def get_times(time_string):    """Split a string of times into background and source intervals.       Usage: times = get_times(time_string)       Input:          time_string -- (string) b1-b2:s1-s2:b3-b4 where b1,b2 are                         the stop and start times of the first                         background interval; s1,s2 are the start and                         stop times of the source interval; and b3,b4                         are the start and stop times of the second                         background interval.       Output:          times -- (list) A list containing the six times that define                   the three intervals.  Returns None if time_string                   cannot be parsed into six floats.    """    match = re.search(r'^(\S+)-(\S+):(\S+)-(\S+):(\S+)-(\S+)$',                      time_string, re.IGNORECASE)    if match:        print 'get_times: {0} is a time string'.format(time_string)        times = []        for i in range(6):            t = match.group(i+1)            if is_number(t):                times.append( float(t) )            else:                print 'get_times: error in time string {0}, {1} is a bad time'.format(time_string, t)                return None    else:        return None            if ( times[1] <= times[0] ) or \            ( times[3] <= times[2] ) or \            ( times[5] <= times[4] ):        print 'get_times: times not increasing in {0}'.format(time_string)        return 'ERROR'            return times####def is_number(s):    """Check if a string is a number."""    try:        float(s)    except ValueError:        return False    else:        return True####def get_detectors(detector_string, trigger_name,workdir):    """Parse the detectors string.       Usage: detector_files = get_detectors(detector_string, trigger_name)              Input:          detector_string -- (string) ALL - Use all the detectors that                             had a detection.                             TRIGGER - Use the detectors that                             triggered.                             nnnnnnnnnnnn - Use detectors specified by                             the user.          trigger_name    -- (string) Fermi/GBM trigger id.       Output:          files -- (list) List of detector files, or None if none were                   found       If detector_string = TRIGGER this function reads the DET_MASK       keyword from the glg_tcat_all file to identify the triggered       detectors.    """        detector_files = get_list_of_detector_files(trigger_name,workdir)    if detector_string == 'ALL':        files = detector_files        return files    elif detector_string == 'TRIGGER':         # Find the tcat file.        working_directory = os.path.abspath(os.path.expanduser(workdir))        files = []        for file in os.listdir(working_directory):            if re.match(r'^glg_tcat_all_' + trigger_name + '_v\d+.fit',                        file, re.IGNORECASE):                tcat_file = os.path.abspath(os.path.expanduser(file))                break        # Read which detectors triggered        fits = pyfits.open(tcat_file, 'readonly')        detmask = fits[0].header['DET_MASK']        fits.close()        files = get_specified_detectors(trigger_name, detector_files, detmask)        return files    elif re.match(r'\d{12}', detector_string):        detmask = detector_string        files = get_specified_detectors(trigger_name, detector_files, detmask)        return files    else:        return None####def get_specified_detectors(trigger_name, detector_files, detmask):    """Create a list of detector files to use.       Usage: files = get_specified_detectors(trigger_name, detector_files,                                              detmask)       Input:          trigger_name   -- (string) Fermi/GBM trigger id.          detector_files -- (list) List of detector files to check.          detmask        -- (string) Which detectors to use.       Output:          files -- (list) List of detector files to use.    """    # Build the list of detectors.    list = []    match = re.search(r'(\d)(\d)(\d)(\d)(\d)(\d)(\d)(\d)(\d)(\d)(\d)(\d)',                      detmask, re.IGNORECASE) # Clumsy!    for i in range(12):        bit = int( match.group(i+1) )        if bit == 1:            if ( i < 10 ):                det = 'n' + str(i)            elif i == 10:                det = 'na'            elif i == 11:                det = 'nb'            else:                print 'warning: get_detectors: bad detector number {0}'.format(i)                det = ''            list.append(det)    files = []    for file in detector_files:        for det in list:            if re.match(r'.+_' + det + '.+.fit', file):                files.append(file)    return files####def get_list_of_detector_files(trigger_name,working_directory):    """Return Fermi/GBM detector files in the current directory.       Usage: files = get_list_of_detector_files(trigger_name)              Input:          trigger_name -- (string) Fermi/GBM trigger id       Output:          files -- (list) List of detector files    """    if(working_directory==None):      working_directory = os.getcwd()    else:      working_directory = os.path.abspath(os.path.expanduser(working_directory))    files = []    for detector_file in os.listdir(working_directory):        if re.match(r'^glg_' + '\w+' + '_\S{2}_' +                    trigger_name + '_v\d+.fit',                    os.path.basename(detector_file), re.IGNORECASE):            files.append(os.path.join(working_directory,detector_file))    return files####def run_gtbin_lightcurve(detector_file, lightcurve_file, tstart, tstop, dtime,ft2="NONE"):    """Call gtbin to generate a Fermi/GBM light curve.           Usage: run_gtbin_lightcurve(detector_file, lightcurve_file,                                   tstart, tstop, dtime)       Input:          detector_file   -- (string) Fermi/GBM detector file name          lightcurve_file -- (string) Output lightcurve file name          tstart          -- (float) Start time for lightcurve in MET s.          tstop           -- (float) Stop time for lightcurve in MET s.          dtime           -- (float) Bin width for lightcurve in s       Output:          Returns None if gtbin ran without generating an error.          Returns an error string if there was a problem.    """        gtbin = GtApp.GtApp('gtbin')    gtbin['evfile'] = detector_file    gtbin['scfile'] = ft2    gtbin['outfile'] = lightcurve_file    gtbin['algorithm'] = 'LC'    gtbin['tbinalg'] = 'LIN'    gtbin['tstart'] = str(tstart)    gtbin['tstop'] = str(tstop)    gtbin['dtime'] = str(dtime)    gtbin['chatter'] = 0    gtbin['clobber'] = 'yes'        try:        gtbin.run()    except:        return 'gtbin failed'    else:        print 'created {0} (dtime={1})'.format(lightcurve_file, dtime)        return None####def time_intervals(lightcurve_file, trigtime):        """Determine background and source intervals.           Usage: intervals = time_intervals(lightcurve_file, trigtime)       Input:          lightcurve_file -- (string) lightcurve file name          trigtime        -- (float) trigger time of the burst       Output:          intervals -- (list) The start and stop times, in MET, of the                       three intervals (first background, source,                       second background.  Returns None if no                       intervals were found       This function finds preliminary intervals using the input light       curve then uses the two background intervals to fit and       subtract the background.  It then finds refined intervals using       the background-subtracted light curve.    """        # Determine the preliminary background and source intervals    intervals = get_time_intervals(lightcurve_file)    if intervals is None:        print 'warning, no source found in {0}, skipping'.format(lightcurve_file)        return None            # Fit and subtract the background from the preliminary light curve    flat_lightcurve_file = subtract_background_curvature(lightcurve_file,                                                         trigtime, intervals)    if flat_lightcurve_file is None:        print 'warning, unable to flatten the lightcurve for {0}'.format(lightcurve_file)        return None            # Determine the background and source intervals from the    # background-subtracted light curve.    intervals = get_time_intervals(flat_lightcurve_file)    if intervals is None:        print 'warning, no source found in {0}, skipping'.format(flat_lightcurve_file)        return None    return intervals####def fit_background(time, counts, trigtime, intervals):    """Fit the background for a light curve.       Usage: background = fit_background(time, counts, trigtime, intervals)       Input:          time      -- (list) time in MET seconds          counts    -- (list) counts corresponding to time          trigtime  -- (float) trigger time of burst          intervals -- (list) the background and source time intervals       Output:          background -- (list) the fitted background corresonding to                         time       This function fits a polynomial to the two background       intervals.  The source interval is not used in the fit.    """    polynomial_order = 2        bkg1_start = intervals[0]    bkg1_stop  = intervals[1]    src_start  = intervals[2]    src_stop   = intervals[3]    bkg2_start = intervals[4]    bkg2_stop  = intervals[5]    # Subtract the trigger time.    t = []    for i in range(len(time)):        t.append( time[i] - trigtime )    # Create arrays with the source removed    tt = []    cc = []    for i in range(len(time)):        if ( time[i] >= bkg1_start ) and ( time[i] < bkg1_stop ):            tt.append(t[i])            cc.append(counts[i])        if ( time[i] >= bkg2_start ) and ( time[i] < bkg2_stop ):            tt.append(t[i])            cc.append(counts[i])                # Fit the background    coeffs = numpy.polyfit(tt, cc, polynomial_order)    # Create the background array    background = []    for i in range( len(t) ):        bkg = 0.0        for j in range( polynomial_order + 1 ):            power = polynomial_order - j            bkg += coeffs[j] * t[i]**power        background.append(bkg)        return background     ####def subtract_background_curvature(lightcurve, trigtime, intervals):    """Fit and subtract curvature from a Fermi/GBM lightcurve.       Usage: new_lightcurve =              subtract_background_curvature(lightcurve, trigtime,                                            intervals)       Input:          lightcurve -- (string) lightcurve filename          trigtime   -- (float) burst trigger time          intervals  -- (list) source and background intervals       Output          new_lightcurve -- (string) background-subtracted lightcurve                            file name       Read the light curve, then fit and subtract the background.       The mean background is added back to the light curve to       preserve Poisson statistics.  The final light curve is written       to a new light curve file.    """    # Get the light curve    lc_file = pyfits.open(lightcurve, 'readonly')    phu         = lc_file[0]    rate_header = lc_file['RATE'].header    rate_data   = lc_file['RATE'].data    gti         = lc_file['GTI']    time   = lc_file['RATE'].data.field('TIME')    counts = lc_file['RATE'].data.field('COUNTS')    error  = lc_file['RATE'].data.field('ERROR')    lc_file.close()         # The last data point can be incomplete, so set it equal to the    # second-to-last data point.    n = len(time) - 1    counts[n] = counts[n-1]    error[n]  = error[n-1]    # Fit the background    background = fit_background(time, counts, trigtime, intervals)    # Compute the mean background and standard deviation    sum = 0.0    for i in range( len(background) ):        sum += background[i]    mean_bkg = sum / len(background)    sum = 0.0    for i in range( len(background) ):        term = (background[i] - mean_bkg)**2        sum += math.sqrt(term)    n = len(background)    var_bkg = sum / ( n - 1 )    sd_bkg = math.sqrt(var_bkg)    # Subtract the fitted trend from each time bin    bkg_subtracted_counts = []    for i in range( len(counts) ):        src = counts[i] - background[i]  + mean_bkg        bkg_subtracted_counts.append(src)    # Create a new HDU with trend-subtracted counts.    # Bewarned and beware: get_coldefs() is depreciated in    # pyfits 3.     #cdefs = rate_data.columns # pyfits 3.0    cdefs = lc_file['RATE'].get_coldefs() # pyfits 2.1.1    #print cdefs.info    # Bewarned and beware: there is a bug in pyfits 2.2.2 and earlier    # that causes del_col to raise a KeyError exception when deleting    # a column from a ColDefs object.  See    # <http://www.stsci.edu/resources/software_hardware/pyfits/release>.    #cdefs.del_col('COUNTS') # pyfits 3.0    #cdefs.change_name('COUNTS', 'OCOUNTS') # pyfits 2.1.1    for i in range( len(counts) ):        rate_data.field('COUNTS')[i] = bkg_subtracted_counts[i]    #print cdefs.info    #col = pyfits.Column(name='COUNTS', format='J', unit='Counts',    #                    array=bkg_subtracted_counts)    # Bewarned and beware: the behaviour of add_col changed in pyfits    # 2.3.    #cdefs.add_col(col) # pyfits 2.3    #new_rate_hdu = pyfits.new_table(cdefs, header=rate_header)    #new_cdefs = cdefs.add_col(col) # before pyfits 2.3    #print cdefs.info    #new_rate_hdu = pyfits.new_table(new_cdefs, header=rate_header)        # Write the trend-subtracted light curve to a new FITS file.    #lc_file['RATE'] = new_rate_hdu    lc_file['RATE'].data = rate_data    #lc_file.info()    new_lightcurve = lightcurve.replace('.lc', '_flatbkg.lc')    # Update keywords    lc_file[0].header['FILENAME'] = new_lightcurve    lc_file[0].header['CREATOR'] = 'do_gbm.py'    lc_file['RATE'].header['CREATOR'] = 'do_gbm.py'    if os.path.exists(new_lightcurve):        os.remove(new_lightcurve)    try:        lc_file.writeto(new_lightcurve)    except ValueError:        #print 'warning, apparently meaningless python ValueError exception when writing to {0} is being ignored in {1}'.format(new_lightcurve, __name__)        pass    except:        print 'subtract_background_curvature: error writing new lightcurve file {0}'.format(new_lightcurve)        return None    finally:        return new_lightcurve####def get_time_intervals(event_file):    """Return background and source start and stop times intervals.       Usage: (bkg1_start,bkg2_stop,src_start,src_stop               bkg2_start,bkg2_stop) =              get_time_intervals(event_file)       Input:          event_file -- (string) Fermi/GBM detector event file name       Output:          bkg1_start -- (float) background interval 1 start time (MET)          bkg1_stop  -- (float) background interval 1 stop time (MET)          src_start  -- (float) source interval start time (MET)          src_stop  -- (float) source interval stop time (MET)          bkg2_start -- (float) background interval 2 start time (MET)          bkg2_stop  -- (float) background interval 2 stop time (MET)      Call gtburstfit to determine Bayesian blocks for the lightcurve.      The first background interval is the first Bayesian block.  The      last time interval is the last Bayesian block unless it is      shorter than min_block_duration, then it is the last two      Bayesian blocks.  The source interval is the interval between      the two background intervals.    """        # This is the minimum allowed duration of the second background    # region.    min_block_duration = 1.0 # s  # Just a guess!        gtburstfit = GtApp.GtApp('gtburstfit')    #This is to correct a problem in the ScienceTools: the executable    #is called gtburstFit, but the parfile is gtburstfit.par (note    #the capital F)    gtburstfit.app = 'gtburstFit'    gtburstfit['evfile'] = event_file    gtburstfit['fitguess'] = 'AUTO'    gtburstfit['amp'] = 0.0    gtburstfit['time0'] = 0.0    gtburstfit['tau1'] = 0.0    gtburstfit['tau2'] = 0.0    gtburstfit['bckgnd'] = 0.0    gtburstfit['evtable'] = 'RATE'    gtburstfit['fit'] = 'NO'    gtburstfit['plotres'] = 'NO'    gtburstfit['ncpprior'] = 9.0    gtburstfit['plot'] = 'NO'    gtburstfit['title'] = 'DEFAULT'    gtburstfit['chatter'] = 2    gtburstfit['clobber'] = 'yes'    gtburstfit['debug'] = 'no'    gtburstfit['gui'] = 'no'        log_file = open('TT_gtburstfit.LOG', 'w')        try:        input, output = gtburstfit.runWithOutput()        for line in output:            if line.find(gtburstfit.appName) != -1:              print line.strip("\n")            log_file.write(line)            if line.find("at the top level:") != -1:                input.close()                output.close()                raise RuntimeError, gtburstfit.appName + " execution failed"    except:        print 'warning, unable to fit the light curve for: {0}'.format(event_file)        return None    finally:        input.close()        output.close()        log_file.close()    log_file = open('TT_gtburstfit.LOG', 'r')    blocks = []    i = 0    found_bayesian_blocks = False    for line in log_file.readlines():        if not found_bayesian_blocks:            if 'Bayesian Blocks' in line:                found_bayesian_blocks = True        else:            m = re.match(r'\[(\S+),\s*(\S+)\]', line, re.IGNORECASE)            if m is not None:                blocks.append( (m.group(1), m.group(2)) )                i += 1    log_file.close()    i -= 1        # The definitions of the background interval and source interval    # are the same as those used by the Swift/BAT tool battblocks.    bkg1_start = float( blocks[0][0] ) # First Bayesian block is the    bkg1_stop  = float( blocks[0][1] ) # first background interval    src_start  = float( blocks[0][1] ) # Source interval is from end of 1st    src_stop   = float( blocks[i][0] ) # Bayesian block to start of last.    # If the last Bayesian block is less than some minimum duration    # then use the second-to-last Bayesian block to determine the stop    # time of the source.    last_block_duration = float(blocks[i][1]) - float(blocks[i][0])    if (  last_block_duration < min_block_duration ):        src_stop = float(blocks[i-1][0])    # Set the second background interval to be from the source stop    # time to the end of the last Bayesian block.    bkg2_start = src_stop    bkg2_stop  = float( blocks[i][1] )        return [bkg1_start, bkg1_stop, src_start, src_stop, bkg2_start, bkg2_stop]####def compute_signal_to_noise(lightcurve_file, intervals, user_times, trigtime):        """Compute the signal-to-noise ratio of a lightcurve.       Usage: sn = compute_signal_to_noise(lightcurve_file, intervals                                           user_times, trigtime)       Input:          lightcurve_file -- (string) name of the lightcurve file          intervals       -- (list) start and stop times of the source                             and background intervals          user_times      -- (boolean) True if the intervals were                             set by the user and False if the                             intervals were determined by the script          trigtime        -- (float) trigger time (MET s)       Output:          mean_sn         -- (float) the mean sigmal-to-noise ratio per                             bin in the source interval          intervals       -- (list) improved source and background                             intervals       This function reads the lightcurve file then fits and subtracts       the background.  The signal-to-noise ratio is computed in each       bin in the source interval then a mean signal-to-noise ratio is       computed.  If user_times is False (the user did not specify the       intervals) then the source interval is refined so that it ends       when max_consecutive_negative_sn bins with negative signal-to-       noise ratios are encountered.  The second backgroud region is       also adjusted.    """    # This is the maximum number of consecutive bins that can have    # negative signal-to-noise ratios before the source is assumed to    # have ended.  This parameter is ignored if the user specified    # the source interval.    max_consecutive_negative_sn = 3    bkg1_start = intervals[0]    bkg1_stop  = intervals[1]    src_start  = intervals[2]    src_stop   = intervals[3]    bkg2_start = intervals[4]    bkg2_stop  = intervals[5]    # Get the light curve    f = pyfits.open(lightcurve_file, 'readonly')    time    = f['RATE'].data.field('TIME')    counts  = f['RATE'].data.field('COUNTS')    error   = f['RATE'].data.field('ERROR')    f.close()        # Compute the background counts    background = fit_background(time, counts, trigtime, intervals)        # Compute the signal-to-noise spectrum    n = 0    sum = 0.0    src_end_time = src_stop    # Skip the last time bin because it may be truncated.    consecutive_negative_sn = 0    for i in range( len(time) - 1 ):        if ( time[i] >= src_start and time[i] <= src_stop ):            sn = ( counts[i] - background[i] ) / error[i]            if sn < 0.0:                consecutive_negative_sn += 1            if consecutive_negative_sn > max_consecutive_negative_sn:                dt = src_stop - time[i]                print 'warning, source appears to have stopped {0} s before the fitted end of the burst ({1} s)'.format(dt, src_stop)                src_end_time = time[i]                if not user_times:                    print 'adjusting the source stop time to {0} (MET)'.format(src_end_time)                break # The source has ended, stop summing            n += 1            sum += sn        elif time[i] > src_stop:            break    mean_src_sn = sum / n        if mean_src_sn < 0.0:        print 'warning, source S/N < 0 (S/N = {0}'.format(mean_src_sn)        print '         sum={0}  n={1}'.format(sum, n)    if not user_times:        intervals[3] = src_end_time # Revised source end time        intervals[4] = src_end_time # Revise 2nd bkgd interval start time    return [mean_src_sn, intervals]####def compute_duration(lightcurve_file, src_start, src_stop,                     bkg_start, bkg_stop, fraction):    """Compute the T_fraction duration of the source.       Usage: [t_fractiont_fraction, t_fraction_err,              t_fraction_start, t_fraction_stop] =              compute_duration(lightcurve_file, src_start, src_stop,                               bkg_start, bkg_stop, fraction)       Input:          lightcurve_file -- (string) Name of lightcurve file          src_start       -- (float) Start time of source (MET)          src_stop        -- (float) Stop time of source (MET)          bkg_start       -- (float) Start time of background (MET)          bkg_stop        -- (float) Stop time of background (MET)          fraction        -- (float) T_fraction value (0-100)       Output:          t_fraction       -- (float) T_fraction duration (s)          t_fraction_err   -- (float) T_fraction 1-sigma error (s)          t_fraction_start -- (float) Start time of T_fraction                              interval (MET s)          t_fraction_stop  -- (float) Stop time of T_fraction                              interval (MET s)       Computes the T_fraction (e.g. T_90, T_50) duration of a source       as well as its 1-sigma error and the start and stop times of       the T_fraction duration.  This function uses the method       described in the battblocks documentation and the TOTVAR method       to compute the error.    """    # Get the light curve    f = pyfits.open(lightcurve_file, 'readonly')    time   = f['RATE'].data.field('TIME')    counts = f['RATE'].data.field('COUNTS')    error  = f['RATE'].data.field('ERROR')    f.close()    # Compute the mean background counts    n = 0    sum = 0.0    for i in range( len(time) ):        if ( time[i] >= bkg_start and time[i] <= bkg_stop ):            n += 1            sum += counts[i]            bkg = sum / n        if n <= 0:            print 'error: no counts in specified background interval'    # Compute the total source counts    total_src_counts = 0.0    for i in range( len(time) ):        if ( time[i] >= src_start and time[i] <= src_stop ):            total_src_counts += counts[i] - bkg    # Compute the cumulative fraction light curve    sum = 0.0    cumulative_lc = []    f = []    f_error = []    for i in range( len(time) ):        if ( time[i] < src_start ):            cumulative_lc.append(0.0)            f.append(0.0)            f_error.append(0.0)        elif ( time[i] >= src_start and time[i] <= src_stop ):            src_counts = counts[i] - bkg            sum += src_counts            cumulative_lc.append(sum)            frac = cumulative_lc[i] / total_src_counts             f.append(frac)            e = error[i] / total_src_counts            f_error.append(e)        elif ( time[i] > src_stop ):            cumulative_lc.append(total_src_counts)            f.append(1.0)            f_error.append(0.0)        # Compute the T_fraction time and error    offset = (100.0 - fraction ) / 2.0 / 100.0    xp = [offset, 1.0-offset]    yp = numpy.interp(xp, f, time)    t_fraction = yp[1] - yp[0]    # Record the start and stop times of the T_fraction interval.    t_fraction_start = yp[0]    t_fraction_stop = yp[1]    sum = 0    for i in range( len(time) ):        sum += f_error[i]**2    t_fraction_err = math.sqrt(sum)    return [t_fraction, t_fraction_err, t_fraction_start, t_fraction_stop]####def select_on_energy(infile, emin, emax, outfile):        """Extract a light curve in a user-specified energy band.       Usage: select_on_energy(infile, emin, emax, outfile)       Input:          infile  -- (string) name of input events file          emin    -- (float) minimum energy (keV)          emax    -- (float) maximum energy (keV)          outfile -- (string) name of output events file    """    # Get the EBOUNDS data from the events file.    event_file = pyfits.open(infile, 'readonly')    try:      event_file['EBOUNDS']    except:      #Just copy input file in output file (this is for LLE)      shutil.copyfile(infile,outfile)      return          channel = event_file['EBOUNDS'].data.field('CHANNEL')    e_min   = event_file['EBOUNDS'].data.field('E_MIN')    e_max   = event_file['EBOUNDS'].data.field('E_MAX')    event_file.close()        # Find the channel bounderies corresponding to [emin,emax].    cmin = -1    cmax = -1    for i in range( len(channel) ):        if ( e_min[i] <= emin ) and ( e_max[i] >= emin ):            cmin = channel[i]        if ( e_min[i] <= emax ) and ( e_max[i] >= emax ):            cmax = channel[i]        #in_events = infile + '[EVENTS]'    #expression = 'PHA.GE.' + str(cmin) + ' .AND. PHA.LE.' + str(cmax)    #print("\n\nFSELECT\n\n")    #command = ['fselect',    #           'infile=' + in_events,    #           'outfile=' + outfile,    #           'expr=' + expression,    #           'histkw=YES',    #           'copyall=YES',    #           'keycopy=YES',    #           'clobber=YES']    #try:    #    subprocess.check_call(command)    #except:    #   print 'warning, unable to extract {0} - {1} keV events from: {2}'.format(emin, emax, in_event)    #    print 'expression used: {0}'.format(expression)    #        #Select the events    f = pyfits.open(infile)    mask = (f['EVENTS'].data.field('PHA') >= int(cmin) ) & (f['EVENTS'].data.field('PHA') <= int(cmax))    selectedData = f['EVENTS'].data[mask]    newHDU = pyfits.BinTableHDU(selectedData,f['EVENTS'].header,'EVENTS')    #Copy everything in the outfile, with the new selected table    f[0].writeto(outfile,clobber=True)    for i in range(1,len(f)):      if(f[i].header['EXTNAME']=='EVENTS'):        pyfits.append(outfile,newHDU.data,newHDU.header)      else:        pyfits.append(outfile,f[i].data,f[i].header)    f.close()    return####def create_time_bins_file(bkg_start, bkg_stop, src_start, src_stop,                          fits_file,):    """Create a time bin file for gtbin.       Usage: create_time_bin_file(background_start_time,                                   background_stop_time,                                   source_start_time,                                   source_stop_time,                                   output_file_name)       Input:          background_start_time -- (float) in MET s          background_stop_time  -- (float) in MET s          source_start_time     -- (float) in MET s          source_stop_time      -- (float) in MET s          fits_file             -- (string) name of output time bins                                   file                                                 Call gtbindef to create a FITS file with the background and       source intervals that will be used to extract spectra from       the Fermi/GBM detector files.    """            text_file = fits_file + '.txt'    f = open(text_file, 'w')    print >>f, bkg_start, bkg_stop    print >>f, src_start, src_stop    f.close()    gtbindef = GtApp.GtApp('gtbindef')    gtbindef['bintype'] = 'T'    gtbindef['binfile'] = text_file    gtbindef['outfile'] = fits_file    gtbindef['chatter'] = 2    gtbindef['clobber'] = 'yes'    gtbindef['debug'] = 'NO'    gtbindef['gui'] = 'NO'        try:        gtbindef.run()    except:        print 'warning, subprocess call failed for {0}'.format(fits_file)    os.remove(text_file)    return####def find_response_file(pha_file):        """Find a .rsp file corresponding a .pha file.       Usage: rsp_file = find_response_file(pha_file)       Input:          pha_file -- (string) Name of .pha file        Output:          rsp_file -- (string Name of corresponding .rsp file       This function finds all of the .rsp files in the current       directory.  Then it looks for a .rsp file with exactly the       same name as the .pha except that tte is replaced with cspec.       Example: glg_tte_n6_bn121205000_v01.rsp and                glg_cspec_n6_bn121205000_v01.pha    """    spectrum_root = pha_file[0:pha_file.rfind('.pha')-18]    s_len = len(spectrum_root)    s_id = pha_file[s_len:len(pha_file)]    s_id = s_id[0:s_id.rfind('.pha')-4]    response_root = 'glg_cspec_'                        r_len = len(response_root)        rsp_file = None    working_directory = os.path.dirname(pha_file)    spectrum_files = []    response_files = []    for file in os.listdir(working_directory):        if re.match(r'\S+\.rsp', file, re.IGNORECASE):            response_files.append(os.path.join(working_directory,file))        response_file_found = False    for r in response_files:        if 'cspec' in r:            r_id = os.path.basename(r)[r_len:len(r)]            r_id = r_id[0:r_id.rfind('.rsp')-4]            if r_id == s_id:                response_file_found = True                rsp_file = r            return rsp_file####def fit_model_spectrum(statmethod, xspec_save_file):    """Fit an XSpec model to spectral data.       Usage: fit_model_spectrum(statmethod, xspec_save_file)       Input:          statmethod      -- (string) XSpec statmethod (e.g., chi,                             cstat)          xspec_save_file -- (string) File to save XSpec commands to       This function uses pyxspec to fit an XSpec model.  It assumes       that the data and the model have already been set up.  This       functions just does the actual fitting.    """        # Fit a spectral model.    xspec.Fit.renorm()    xspec_save_file.write('renorm\n')    xspec.Fit.query = 'yes'    xspec_save_file.write('query yes\n')    xspec.Fit.statMethod = statmethod    s = 'statistic ' + statmethod + '\n'    xspec_save_file.write(s)    status=xspec.Fit.perform()    print'status: ',status    xspec_save_file.write('fit\n')            return####def fit_spectra(trigger_name, spectrum_list, response_list, fit_statistic,                e_flu_min, e_flu_max, duration, plot_file):    """Do an XSpec spectral fit to Fermi/GBM burst data.       Usage: [model_data, [goodness_of_fit, dof],              [fluence, fluence_err]] = fit)spectra(trigger_name,              spectrum_list, response_list, fit_statistic              e_flu_min, e_flu_max, duration, plot_file)       Input:          trigger_name  -- (string) Fermi/GBM trigger id          spectrum_list -- (list) list of .pha files to use          response_list -- (list) of .rsp files to use          fit_statistic -- (string) XSpec fit statistic to use          e_flu_min     -- (float) minimum energy for fluence                           calculation (keV)          e_flu_max     -- (float) maximum energy for fluence                           calculation (keV)          duration      -- (float) duration of the spectrum (s)          plot_file     -- (string) name of output plot file       Output:          model_data     -- (list) XSpec model information                            [name, par1, par1_err, par2, par2_err, ...]          goodnes_of_fit -- (float) goodness of fit value          dof            -- (float) degrees of freedom in fit          fluence        -- (float) fluence (erg cm^-2)          fluence_err    -- (float) error in fluence (erg cm^-2)       This function does the spectral fitting using pyxspec.  It       currently tries to fit a Band model and a cutoff power law       model and chooses the one with the best fit.    """    # Set a log file for the output from XSpec    log_file_name = trigger_name + '_xspec.log'    log_file = xspec.Xset.openLog(log_file_name)        # Use the log file, not the screen, for output    xspec.Xset.chatter = 0    # Create a file to save XSpec commands to.    xspec_save_file_name = trigger_name + '_xspec_save' + '.xcm'    xspec_save_file = open(xspec_save_file_name, 'w')    xspec_save_file.write('# XSpec command file\n')    s = '# Created by {0}'.format(__file__)    xspec_save_file.write(s)    s = '# ' + trigger_name + '\n'    xspec_save_file.write(s)    xspec.AllData.clear()    # Read in the data    for i in range( len(spectrum_list) ):        j = i + 1        xspec.AllData += spectrum_list[i]+'{2}'        s = 'data 1:' + str(j) + ' ' + spectrum_list[i] + '{2}\n'        xspec_save_file.write(s)        xspec.AllData(i+1).response = response_list[i]        s = 'response 1:' + str(j) + ' ' + response_list[i] + '\n'        xspec_save_file.write(s)        xspec.AllData(i+1).background = spectrum_list[i]+'{1}'        s = 'backgrnd 1:' + str(j) + ' ' + spectrum_list[i] + '{1}\n'        xspec_save_file.write(s)           xspec.AllData.show()    # Determine the Fermi/GBM detector type.    dettype_list = []    print 'Detector Types:'    for i in range( len(spectrum_list) ):        spectrum_file = spectrum_list[i]        fits = pyfits.open(spectrum_file, 'readonly')        try:          detnam = fits[0].header['DETNAM']        except:          detnam = 'LLE_00'        fits.close()        dettype_list.append(detnam[0:detnam.find('_')])        print '   {0}: {1}'.format(spectrum_file, dettype_list[i])    # Set the good channels.    xspec.Plot.xAxis = 'keV'    # The ignore values are in keV when the X axis has units of energy.    # Ignore values must be decimals cast to strings.    for i in range( len(spectrum_list) ):        if 'NAI' in dettype_list[i]:            ign1 = str(0.0)            ign2 = str(8.0)            ign3 = str(900.0)            ign4 = '**'        elif 'BGO' in dettype_list[i]:            ign1 = str(0.0)            ign2 = str(200.0)            ign3 = str(40000.0)            ign4 = '**'        elif 'LLE' in dettype_list[i]:            ign1 = str(0.0)            ign2 = str(3e4)            ign3 = str(1e7)            ign4 = '**'        else:            print '{0}, error, unknown detector type ({1})'.format(__file__, dettype_list[i])        xspec.AllData(i+1).ignore(ign1 + '-' + ign2 + ',' + ign3 + '-' + ign4)        j = i + 1        s = 'ignore ' + str(j) + ':' + ign1 + '-' + ign2 + \            ' ' + ign3 + '-' + ign4 + '\n'        xspec_save_file.write(s)    # Try fitting both the grbm and the cutoffpl models and see which    # gives the best fit.    print 'fit_spectra: fitting grbm model'    xspec.AllModels.clear()    # Load the GRB model.    model = xspec.Model('grbm')    xspec_save_file.write('model grbm\n')    xspec_save_file.write('-1\n') # alpha    xspec_save_file.write('-2\n') # beta    xspec_save_file.write('300\n') # tem    xspec_save_file.write('1\n') # norm    # Fit the GRB model.    fit_model_spectrum(fit_statistic, xspec_save_file)    grbm_alpha = model.grbm.alpha.values[0]    grbm_beta  = model.grbm.beta.values[0]    grbm_tem   = model.grbm.tem.values[0]    goodness_of_fit_grbm = xspec.Fit.statistic    dof_grbm = xspec.Fit.dof    xspec.Fit.error("1")    xspec_save_file.write('error 1\n')    err = xspec.AllModels(1)(1).error    grbm_alpha_err = ( err[1] - err[0] ) /2.0    xspec.Fit.error("2")    xspec_save_file.write('error 2\n')    err = xspec.AllModels(1)(2).error    grbm_beta_err = ( err[1] - err[0] ) / 2.0    xspec.Fit.error("3")    xspec_save_file.write('error 3\n')    err = xspec.AllModels(1)(3).error    grbm_tem_err = ( err[1] - err[0] ) / 2.0    # Plot the results    plot_file1 = 'grbm_' + plot_file    xspec.Plot.device = plot_file1+'/ps'    xspec.Plot('uf')    # Compute the flux    s = str(e_flu_min) + ' ' + str(e_flu_max) + ' err'    # catch error to continue to second model fit -    try:	status = xspec.AllModels.calcFlux(s)    except Exception, msg:	print msg    xspec_save_file.write(s + '\n')    flux_grbm = xspec.AllData(1).flux    #flux_grbm = [-1, -1, -1]    print 'fit_spectra: fitting cutoffpl model'    xspec.AllModels.clear()    # Load the cutoff power law model.    model = xspec.Model('cutoffpl')    xspec_save_file.write('model cutoffpl\n')    xspec_save_file.write('1\n') # PhoIndex    xspec_save_file.write('15\n') # HighECut    xspec_save_file.write('1\n') # norm    # Fit the model.    fit_model_spectrum(fit_statistic, xspec_save_file)    cutoffpl_alpha = model.cutoffpl.PhoIndex.values[0]    cutoffpl_ec    = model.cutoffpl.HighECut.values[0]    cutoffpl_chi2 = xspec.Fit.statistic    cutoff_pl_dof  = xspec.Fit.dof    goodness_of_fit_cutoffpl = xspec.Fit.statistic    dof_cutoffpl = xspec.Fit.dof    xspec.Fit.error("1")    xspec_save_file.write('error 1\n')    err = xspec.AllModels(1)(1).error    cutoffpl_alpha_err = ( err[1] - err[0] ) /2.0    xspec.Fit.error("2")    xspec_save_file.write('error 2\n')    err = xspec.AllModels(1)(2).error    cutoffpl_ec_err = ( err[1] - err[0] ) / 2.0    #Plot the results    plot_file1 = 'cutoffpl_'+plot_file    xspec.Plot.device = plot_file1+'/ps'    xspec.Plot('uf')    # Compute the flux    s = str(e_flu_min) + ' ' + str(e_flu_max) + ' err'    xspec.AllModels.calcFlux(s)    xspec_save_file.write(s + '\n')    flux_cutoffpl = xspec.AllData(1).flux    #flux_cutoffpl = [-1, -1, -1]    # Pick the best fit    print ''    reduced_fit_stat_grbm = goodness_of_fit_grbm / dof_grbm    reduced_fit_stat_cpl = goodness_of_fit_cutoffpl / dof_cutoffpl    if reduced_fit_stat_grbm < reduced_fit_stat_cpl:        print '==> Band model gives best fit'        best_model = 'grbm'        print ''        print 'The Band model spectral parameters are'        print '    alpha = {0} +/- {1}'.format(grbm_alpha, grbm_alpha_err)        print '     beta = {0} +/- {1}'.format(grbm_beta, grbm_beta_err)        print '   E_peak = {0} +/- {1} keV'.format(grbm_tem, grbm_tem_err)        model_data = [best_model,                      grbm_alpha, grbm_alpha_err,                      grbm_beta, grbm_beta_err,                      grbm_tem, grbm_tem_err]        print ' {0}/dof = {1} / {2}'.format(fit_statistic, goodness_of_fit_grbm, dof_grbm)        flux = flux_grbm[0]        flux_err = ( flux_grbm[2] - flux_grbm[1] ) / 2.0        print '     flux = {0} +/- {1} erg/cm^2/s'.format(flux, flux_err)        fluence = flux * duration        fluence_err = flux_err * duration        print '  fluence = {0} +/- {1} erg/cm^2 ({2} s)'.format(fluence, fluence_err, duration)        goodness_of_fit = goodness_of_fit_grbm        dof = dof_grbm    else:        print '==> Cutoff Power Law gives best fit'        best_model = 'cutoffpl'        print ''        print 'The cutoff power law spectral parameters are'        print '    alpha = {0} +/- {1}'.format(cutoffpl_alpha, cutoffpl_alpha_err)        print '      E_c = {0} +/- {1} keV'.format(cutoffpl_ec, cutoffpl_ec_err)        model_data = [best_model,                      cutoffpl_alpha, cutoffpl_alpha_err,                      cutoffpl_ec, cutoffpl_ec_err]        print ' {0}/dof = {1} / {2}'.format(fit_statistic, goodness_of_fit_cutoffpl, dof_cutoffpl)        flux = flux_cutoffpl[0]        flux_err = ( flux_cutoffpl[2] - flux_cutoffpl[1] ) / 2.0        print '     flux = {0} +/- {1} erg/cm^2/s'.format(flux, flux_err)        fluence = flux * duration        fluence_err = flux_err * duration        print '  fluence = {0} +/- {1} erg/cm^2 ({2} s)'.format(fluence, fluence_err, duration)        goodness_of_fit = goodness_of_fit_cutoffpl        dof = cutoff_pl_dof    print ''    # Close the XSpec log file    xspec.Xset.closeLog()    xspec_save_file.close()    print ''    print 'XSpec commands are stored in {0}'.format(xspec_save_file_name)    print ''    return [model_data, [goodness_of_fit, dof], [fluence, fluence_err]]#########################################################################def main(*arguments,**kwargs):    """Run basic GRB processing on Fermi/GBM data.    This script takes a Fermi/GBM burst trigger directory as input.    It constructs light curves, identifies the burst, computes burst    durations, and fits a spectrum to the burst.  The analysis is    fully automated and is intended to provide a quick look analysis    of a Fermi/GBM trigger.    Usage:    do_gbm.py trigger_id timestring detectors timezero    trigger_id -- The Fermi/GBM trigger id of this burst.    timestring -- NONE tells the script to identify the source                  and background intervals automatically.                  b1-b2:s1-s2:b3-b4 where b1, b2, s1, s2, b3, b4                  are the start and stop times of the first                  background interval (b1,b2), the source interval                  (s1,s2), and the second background interval                  (b3,b4) respectively, in MET seconds.    detectors  -- ALL tells the script to use all the detectors,                  TRIGGER tells the script to use only the                  detectors that were triggered by the burst,                  nnnnnnnnnnnn is a string of 0s and 1s where                  1 indicates that this detector is to be used.                  The string starts with the n0 detector and ends                  with the nb detector.    timezero   -- TRIGTIME tells the script to set the trigger time                  to the value of the TRIGTIME keyword in the input                  detector file.                  x.y tells the script to use the time x.y (MET                  seconds) as the trigger time.    """    version = '0.1.1 beta'    date = '2011-12-16'    print    print '==> {0}'.format(__file__)    print '    version {0}    date {1}'.format(version, date)     print ''        # Energy range for T_90 calculation.    e_t90_min =  50 # keV    e_t90_max = 300 # keV    # Energy range for fluence calculation.    e_flu_min =   10 # keV    e_flu_max = 1000 # keV    # Get command line arguments    if len(arguments) < 5:        message = 'usage: {0} trigger_name bkg1_start-bkg1_stop:src_start-src_stop:bkg2_start-bkg2_stop detectors timezero'.format(__file__)        raise RuntimeError(message)    trigger_name = arguments[0]    time_string = arguments[1]    detector_string = arguments[2]    timezero = arguments[3]    if(len(arguments)==5):      workdir = arguments[4]    else:      workdir = None    pass        datasets = None    doSpectra = True    for key in kwargs.keys():      if  key.lower()=='datasets':    datasets = kwargs[key]      elif key.lower()=='dospectra':  doSpectra = bool(kwargs[key])    times = get_times(time_string)    if times == None:        user_times = False    elif times == 'ERROR':        message = 'error, bad user-supplied time intervals'        raise RuntimeError(message)    else:        user_times = True        print 'using user-supplied time intervals'    intervals = times # For now assume that the user-supplied times are in MET.        if(detector_string[0]!='0' and detector_string[0]!='1' and       detector_string!='ALL' and detector_string!='TRIGGER' and        detector_string!='DATASETS'):         #The user provided already a list of files         files = map(lambda x:os.path.abspath(os.path.expandvars(os.path.expanduser(x))),                     detector_string.split(","))    elif(detector_string=='DATASETS'):      if(datasets!=None):        #User-provided datasets        files            = map(lambda x:x['eventfile'],datasets)        rspFiles         = map(lambda x:x['rspfile'],datasets)        ft2files         = map(lambda x:x['ft2file'],datasets)      else:        raise RuntimeError("If you specify DATASETS, then you have to provide them using the dataset keyword")    else:      files = get_detectors(detector_string, trigger_name, workdir)    if len(files) < 1:        message = 'error, no detector files found for trigger {0}'.format(trigger_name)        raise ValueError(message)    if timezero == 'TRIGTIME':        print 'timezero = TRIGTIME from detector files'    else:        print 'timezero = {0} (s; MET)'.format(timezero)    # Set up some working lists    sn_ratio = []    t100 = []    bin_width = []    print ''    print '*****'    print 'Construct light curves for {0}'.format(trigger_name)    print ''        # Run gtbin on each detector data file to create light curves and    # PHA2 files.    file_number = -1    tstarts = []    tstops = []    for fid,detector_file in enumerate(files):        print ''        print 'working on {0}'.format(detector_file)                file_number += 1        # Get time information from this detector file        fitsfile = pyfits.open(detector_file, 'readonly')        tstart   = min(fitsfile['EVENTS'].data.field('TIME'))        tstop    = max(fitsfile['EVENTS'].data.field('TIME'))        tstarts.append(tstart)        tstops.append(tstop)        if timezero == 'TRIGTIME':            trigtime = fitsfile['EVENTS'].header['TRIGTIME']        else:            trigtime = float(timezero)        print 'setting TRIGTIME = {0} s (MET)'.format(timezero)                fitsfile.close()        file_root = detector_file[0:detector_file.rfind('.fit')]        i = detector_file.find('_') + 1        j = detector_file.find('_',i)        data_type = file_root[i:j]        # Time bin width        # Valerie's tutorial says        #      tte: 0.128 s default        #    cspec: 1.024 s default after TRIGTIME        #    ctime: 0.064 s default        if data_type == 'tte':            default_dtime = 0.128 # seconds        elif data_type == 'cspec':            default_dtime = 1.024 # seconds        elif data_type == 'ctime':            default_dtime = 0.064 # seconds        elif data_type == 'lle':            default_dtime = 1.024 # seconds            tstart = trigtime-50            tstop = trigtime+300        else:            nbins = 1024            default_dtime = ( tstop - tstart ) / nbins # Arbitrary bin size            print 'warning, unknown data type ({0})'.format(data_type)            print 'warning, arbitrarily binning data into {0} bins'.format(nbins)        dtime = default_dtime        print 'light curve bin width: {0} s'.format(dtime)        # Create the light curve file        lightcurve_file = os.path.basename(detector_file).replace('.fit', '.lc')        lightcurve_file = os.path.abspath(os.path.join(os.getcwd(),lightcurve_file))        status = run_gtbin_lightcurve(detector_file, lightcurve_file,                                      tstart, tstop, dtime)        if status is not None:            print 'warning, failed to generate light curve for {0}'.format(detector_file)            continue # Skip this detector                # Determine the background and source intervals.        if user_times:            duration = intervals[3] - intervals[2]            print 'using user-defined time intervals'        else:            # Determine the intervals            intervals = time_intervals(lightcurve_file, trigtime)            if intervals is None:                print 'warning, unable to determine source and background intervals for {0}'.format(lightcurve_file)                sn_ratio.append(-1)                t100.append(-1)                bin_width.append(-1)                continue # Skip this detector            else:                src_start = intervals[2]                src_stop  = intervals[3]                duration = src_stop - src_start        print 'preliminary burst duration = {0} s'.format(duration)        # Compute the signal to noise of the source part of the spectrum.        list = compute_signal_to_noise(lightcurve_file, intervals,                                       user_times, trigtime)        mean_sn = list[0]        intervals = list[1]        print 'mean S/N per bin = {0}'.format(mean_sn)        sn_ratio.append( mean_sn )        src_start = intervals[2]        src_stop = intervals[3]        duration = src_stop - src_start        print 'refined burst duration = {0} s'.format(duration)        t100.append( duration )        bin_width.append( dtime )    print ''    print '*****'    print 'Find the best light curves for {0}'.format(trigger_name)    print ''    for i in range( len(files) ):        print '{0}: {1}  S/N = {2}   t = {3} s'.format(i, files[i], sn_ratio[i], t100[i])    print ''    # Determine which light curve has the best signal-to-noise    # ratio for the source.    threshold_sn = 1.5    print 'S/N per bin threshold: {0}'.format(threshold_sn)     n_good = 0    i_best = 0    max_sn = sn_ratio[0]    for i in range( len(sn_ratio) ):        if sn_ratio[i] > threshold_sn:            n_good += 1            if sn_ratio[i] > max_sn:                max_sn = sn_ratio[i]                i_best = i    if n_good > 0:        best_event_file = files[i_best]        print 'Best source S/N: {0}  S/N={1}'.format(files[i_best], max_sn)    else:        message = 'error, no light curves with S/N > {0}'.format(threshold_sn)        raise RuntimeError(message)    # Create a new events file and corresponding light curve using the    # events in the T90 energy range (keV) from the best events    # file.    suffix = '_' + str(e_t90_min) + '_' + str(e_t90_max) + '.evt'    event_file = best_event_file.replace('.fit', suffix)    select_on_energy(best_event_file, e_t90_min, e_t90_max, event_file)        # Create the light curve file from events in the specified energy    # range.    #Ensure to use a global time interval covered by all detectors    tstart = max(tstarts)    tstop = min(tstops)    lightcurve_file = os.path.basename(event_file).replace('.evt', '.lc')    lightcurve_file = os.path.abspath(os.path.join(os.getcwd(),lightcurve_file))    status = run_gtbin_lightcurve(event_file, lightcurve_file,                                  tstart, tstop, dtime)    if status is not None:        message = 'warning, failed to generate light curve for {0}'.format(detector_file)        raise RuntimeError(message)    # Determine the background and source intervals.    if user_times:        duration = intervals[3] - intervals[2]        print 'using user-defined time intervals'    else:        # Determine the intervals        intervals = time_intervals(lightcurve_file, trigtime)        if intervals is None:            message = 'warning, unable to determine source and background intervals for {0}'.format(lightcurve_file)            raise RuntimeError(message)        else:            src_start = intervals[2]            src_stop  = intervals[3]            duration = src_stop - src_start    print 'burst duration = {0} s'.format(duration)    # Set the background and source intervals using the detector with    # the best light curve.    fits_bin_file = event_file.replace('.evt', '_tbins.fits')    b1_start = intervals[0]    b1_stop  = intervals[1]    s_start = intervals[2]    s_stop  = intervals[3]    create_time_bins_file(b1_start, b1_stop, s_start, s_stop, fits_bin_file)    duration = s_stop - s_start    print 'burst duration ({0}-{1} keV): {2} s'.format(e_t90_min, e_t90_max, duration)    # Compute the T90 duration.`<    t = compute_duration(lightcurve_file, s_start, s_stop,                         b1_start, b1_stop, 90)    t90       = t[0]    t90_err   = t[1]    t90_start = t[2] - trigtime    t90_stop  = t[3] - trigtime    print 'burst T_90: {0} +/- {1} s  from {2} to {3}'.format(t90, t90_err, t90_start, t90_stop)    print ''    print 'Time Intervals'    print '   1st background start: {0}'.format(intervals[0]-trigtime)    print '   1st background stop:  {0}'.format(intervals[1]-trigtime)    print '           source start: {0}'.format(intervals[2]-trigtime)    print '           source stop:  {0}'.format(intervals[3]-trigtime)    print '   2nd background start: {0}'.format(intervals[4]-trigtime)    print '   2nd background stop:  {0}'.format(intervals[5]-trigtime)    print ''        if(XSPEC_AVAILABLE and doSpectra):      print ''      print '*****'      print 'Extract the spectra for {0}'.format(trigger_name)      print ''      # Extract spectra for each light curve where a source was found.      for fid,detector_file in enumerate(files):          print ''          print 'working on {0}'.format(detector_file)            file_root = detector_file[0:detector_file.rfind('.fit')]          # Create the PHA2 files          pha_file = file_root + '.pha'                    if(datasets!=None):            rsp    = rspFiles[fid]            scfile = ft2files[fid]          else:            rsp       = find_response_file(pha_file)            scfile = 'NONE'          data      = dataHandling.LLEData(detector_file,rsp,scfile)          try:            data.binByEnergyAndTime(fits_bin_file,pha_file)          except:              print 'warning, cannot obtain spectra for %s' %(detector_file)              print ''              continue          print 'created spectrum {0}'.format(pha_file)          print ''        print ''      print '*****'      print 'Find the best spectra for {0}'.format(trigger_name)      print ''            # For now just create a list of responses manually      response_list_ = []      if(datasets!=None):        response_list_ = rspFiles      else:        for spectrum in spectrum_list:            response = find_response_file(spectrum)            response_list_.append(response)        pass      pass            # Determine which light curves have a mean S/N per bin      # for the source that is above some threshold                  spectrum_list = []      response_list = []      for i in range( len(sn_ratio) ):           if ( sn_ratio[i] > threshold_sn ):               pha_file = files[i].replace('.fit', '.pha')               spectrum_list.append(pha_file)               response_list.append(response_list_[i])               print '{0} has S/N = {1}'.format(pha_file, sn_ratio[i])           #if ( '_b0_' in files[i] ) or  ( '_b1_' in files[i] ):           #    file_root = files[i][0:files[i].rfind('.fit')]           #    pha_file = file_root + '.pha'           #    spectrum_list.append(pha_file)           #    print '{0} has S/N = {1}'.format(pha_file, sn_ratio[i])        # Add the response matrix data to the PHA file.      #update_pha_keywords() Is this the right approach              print ''      print '#   Spectrum                          Response'      for i in range( len(spectrum_list) ):          print '{0}:  {1}    {2}'.format(i, spectrum_list[i], response_list[i])      # The right solution is probably to put the      # response matrix and background file information      # into the FITS header.              # Do the XSpec analysis.          print ''      print '*****'      print 'Perform the joint spectral fit for {0}'.format(trigger_name)      print ''            fit_statistic = 'cstat'  # Valerie's tutorial says to use cstat.      print 'The fitting statistic is {0}'.format(fit_statistic)        spectra_plot_file = trigger_name+'_xspec.ps'      spectrum_results = fit_spectra(trigger_name, spectrum_list,                                     response_list, fit_statistic,                                     e_flu_min, e_flu_max,                                     duration, spectra_plot_file)      model_data = spectrum_results[0]      model_type = model_data[0]      if model_type == 'grbm':          model = 'Band Function'          alpha     = model_data[1]          alpha_err = model_data[2]          beta      = model_data[3]          beta_err  = model_data[4]          epeak     = model_data[5] * (2.0 + alpha)          epeak_err = math.sqrt( (2.0 + alpha)**2 * model_data[6]**2 +                                 model_data[5]**2 * alpha_err**2 )      elif model_type == 'cutoffpl':          model = 'Power Law with Exponential Cutoff'          alpha     = model_data[1]          alpha_err = model_data[2]          epeak     = model_data[3] * (2.0 + alpha)          epeak_err = math.sqrt( (2.0 + alpha)**2 * model_data[4]**2 +                                 model_data[3]**2 * alpha_err**2 )      gof_data        = spectrum_results[1]      goodness_of_fit = gof_data[0]      dof             = gof_data[1]      fluence_data = spectrum_results[2]      fluence      = fluence_data[0]      fluence_err  = fluence_data[1]    # Tell the user what the results are    print ''    print '*****'    print '*****'    print 'Summary Information for {0}'.format(trigger_name)    print ''    fitsfile = pyfits.open(best_event_file, 'readonly')    trigtime = fitsfile['EVENTS'].header['TRIGTIME']    object   = fitsfile['EVENTS'].header['OBJECT']    fitsfile.close()    print '                 Source: {0}'.format(object)    print '           Trigger Time: {0} (MET)'.format(trigtime)    if timezero != 'TRIGTIME':        print '          Used Timezero: {0} (MET)'.format(timezero)    print '         Best Data File: {0}'.format(best_event_file)    print 'Signal-to-Noise Per Bin: {0:0.2f}'.format(max_sn)    print '                   T_90: {0:0.3f} +/- {1:0.3f} s ({2}-{3} keV)'.format(t90, t90_err, e_t90_min, e_t90_max)        if(XSPEC_AVAILABLE and doSpectra):      print '               Spectrum: {0}'.format(model)      if model_type == 'grbm':          print '                  alpha: {0:5.2f} +/- {1:5.2f}'.format(alpha, alpha_err)          print '                   beta: {0:5.2f} +/- {1:5.2f}'.format(beta, beta_err)          print '                 E_peak: {0:0.1f} +/- {1:0.1f} keV'.format(epeak, epeak_err)      elif model_type == 'cutoffpl':          print '                  alpha: {0:5.2f} +/- {1:5.2f}'.format(alpha, alpha_err)          print '                 E_peak: {0:0.1f} +/- {1:0.1f} keV'.format(epeak, epeak_err)      print '  fluence ({0}-{1} keV): {2:8.2e} +/- {3:8.2e} erg/cm^-2'.format(e_flu_min, e_flu_max, fluence, fluence_err)      print ''      print 'goodness of fit / dof ({0}) : {1} / {2}'.format(fit_statistic, goodness_of_fit, dof)      print ''    pass    return t90, t90_err####if __name__ == '__main__':    main(*sys.argv[1:])